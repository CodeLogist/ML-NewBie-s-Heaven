{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anna_KaRNNa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwd2MibbbSp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3E5Vaf9fskn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('anna.txt','r') as f:\n",
        "  text=f.read()\n",
        "  vocab=sorted(set(text))\n",
        "  vocab_to_int={c:i for i,c in enumerate(vocab)}\n",
        "  int_to_vocab=dict(enumerate(vocab))\n",
        "  encoded=np.array([vocab_to_int[c] for c in text],dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJhvueKRh2Qs",
        "colab_type": "code",
        "outputId": "af39fdda-bfa0-440a-a004-07057ad66ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "print(text[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everythin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWg0eZHOiATV",
        "colab_type": "code",
        "outputId": "b3638e3c-75c3-4986-915b-6167fc71958a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
              "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
              "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70, 64,\n",
              "       57, 72, 72, 81,  1, 62, 57, 69, 65, 68, 81,  1, 65, 75,  1, 77, 70,\n",
              "       64, 57, 72, 72, 81,  1, 65, 70,  1, 65, 76, 75,  1, 71, 79, 70,  0,\n",
              "       79, 57, 81, 13,  0,  0, 33, 78, 61, 74, 81, 76, 64, 65, 70],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl4oV2YxibLC",
        "colab_type": "text"
      },
      "source": [
        "Type of a Classification Problem:\n",
        "      num_classes=len()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fodCTVnwiTlF",
        "colab_type": "code",
        "outputId": "8e31996c-2c7c-4684-b271-0ac69882de2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRQocypCi5fa",
        "colab_type": "text"
      },
      "source": [
        "##Mini-Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGNRTeMHixce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr,batch_size,n_steps):\n",
        "  characters_per_batch=batch_size*n_steps\n",
        "  n_batches=len(arr)//characters_per_batch\n",
        "  \n",
        "  arr=arr[:(characters_per_batch*n_batches)]\n",
        "  \n",
        "  arr=arr.reshape(batch_size,n_steps*n_batches)\n",
        "  \n",
        "  for n in range(0,arr.shape[1],n_steps):\n",
        "    x=arr[:,n:n+n_steps]\n",
        "    y_temp=arr[:,n+1:n+n_steps+1]#but what about last batch\n",
        "    y=np.zeros(x.shape,dtype=x.dtype)\n",
        "    y[:,:y_temp.shape[1]]=y_temp #adds appropriate number of zeros in last batch\n",
        "    yield x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soHabusWnkc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches=get_batches(encoded,10,50)\n",
        "x,y=next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNWU-e_Jvs_k",
        "colab_type": "code",
        "outputId": "38484d0e-b6dc-4b32-8de8-623415fd6cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "int_to_vocab[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlaGkV5kpM6V",
        "colab_type": "code",
        "outputId": "e91e39f1-75e9-4c3c-8008-e40730463812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "print('x\\n',x[:10,:10])\n",
        "print('y\\n',y[:10,:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[31 64 57 72 76 61 74  1 16  0]\n",
            " [ 1 57 69  1 70 71 76  1 63 71]\n",
            " [78 65 70 13  0  0  3 53 61 75]\n",
            " [70  1 60 77 74 65 70 63  1 64]\n",
            " [ 1 65 76  1 65 75 11  1 75 65]\n",
            " [ 1 37 76  1 79 57 75  0 71 70]\n",
            " [64 61 70  1 59 71 69 61  1 62]\n",
            " [26  1 58 77 76  1 70 71 79  1]\n",
            " [76  1 65 75 70  7 76 13  1 48]\n",
            " [ 1 75 57 65 60  1 76 71  1 64]]\n",
            "y\n",
            " [[64 57 72 76 61 74  1 16  0  0]\n",
            " [57 69  1 70 71 76  1 63 71 65]\n",
            " [65 70 13  0  0  3 53 61 75 11]\n",
            " [ 1 60 77 74 65 70 63  1 64 65]\n",
            " [65 76  1 65 75 11  1 75 65 74]\n",
            " [37 76  1 79 57 75  0 71 70 68]\n",
            " [61 70  1 59 71 69 61  1 62 71]\n",
            " [ 1 58 77 76  1 70 71 79  1 75]\n",
            " [ 1 65 75 70  7 76 13  1 48 64]\n",
            " [75 57 65 60  1 76 71  1 64 61]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV9Nkr7WrGo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_inputs(batch_size,num_steps):\n",
        "  inputs=tf.placeholder(tf.int32,[batch_size,num_steps],name='inputs')\n",
        "  targets=tf.placeholder(tf.int32,[batch_size,num_steps],name='targets')\n",
        "  keep_prob=tf.placeholder(tf.float32,name='keep_prob')\n",
        "  return inputs,targets,keep_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMRNmNmrzecH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_lstm(lstm_size,num_layers,batch_size,keep_prob):\n",
        "  \n",
        "  def build_cell(lstm_size,keep_prob):\n",
        "    lstm=tf.nn.rnn_cell.LSTMCell(num_units=lstm_size)\n",
        "    drop=tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
        "    return drop\n",
        "  cell=tf.nn.rnn_cell.MultiRNNCell([build_cell(lstm_size,keep_prob) for _ in range(num_layers)])\n",
        "  initial_state=cell.zero_state(batch_size,tf.float32)\n",
        "  return cell,initial_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgpNAUxAR4r0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_output(lstm_output,in_size,out_size):\n",
        "  seq_output=tf.concat(lstm_output,axis=1)\n",
        "  x=tf.reshape(seq_output,[-1,in_size])\n",
        "\n",
        "  with tf.variable_scope('softmax'):\n",
        "    softmax_w=tf.Variable(tf.truncated_normal((in_size,out_size),stddev=0.1))\n",
        "    softmax_b=tf.Variable(tf.zeros(out_size))\n",
        "    \n",
        "    logits=tf.matmul(x,softmax_w)+softmax_b\n",
        "    \n",
        "    out=tf.nn.softmax(logits,name='predictions')\n",
        "    \n",
        "    return out,logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qghjhMAyND-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_loss(logits,targets,lstm_size,num_classes):\n",
        "  y_one_hot=tf.one_hot(targets,num_classes)\n",
        "  y_reshaped=tf.reshape(y_one_hot,logits.get_shape())\n",
        "  \n",
        "  loss=tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
        "  loss=tf.reduce_mean(loss)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdB1QV6D6X8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_optimizer(loss, learning_rate, grad_clip):\n",
        "    ''' Build optmizer for training, using gradient clipping.\n",
        "    \n",
        "        Arguments:\n",
        "        loss: Network loss\n",
        "        learning_rate: Learning rate for optimizer\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
        "    tvars = tf.trainable_variables()\n",
        "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
        "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
        "    \n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfYakULW3JEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100        # Sequences per batch\n",
        "num_steps = 100         # Number of sequence steps per batch\n",
        "lstm_size = 512         # Size of hidden layers in LSTMs\n",
        "num_layers = 2          # Number of LSTM layers\n",
        "learning_rate = 0.01   # Learning rate\n",
        "keep_prob = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMLtEVXa5IbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN:\n",
        "    \n",
        "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
        "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
        "                       grad_clip=5, sampling=False):\n",
        "    \n",
        "        # When we're using this network for sampling later, we'll be passing in\n",
        "        # one character at a time, so providing an option for that\n",
        "        if sampling == True:\n",
        "            batch_size, num_steps = 1, 1\n",
        "        else:\n",
        "            batch_size, num_steps = batch_size, num_steps\n",
        "\n",
        "        tf.reset_default_graph()\n",
        "        \n",
        "        # Build the input placeholder tensors\n",
        "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
        "\n",
        "        # Build the LSTM cell\n",
        "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
        "\n",
        "        ### Run the data through the RNN layers\n",
        "        # First, one-hot encode the input tokens\n",
        "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
        "        \n",
        "        # Run each sequence step through the RNN and collect the outputs\n",
        "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
        "        self.final_state = state\n",
        "        \n",
        "        # Get softmax predictions and logits\n",
        "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
        "        \n",
        "        # Loss and optimizer (with gradient clipping)\n",
        "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
        "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj40fCCm3sSq",
        "colab_type": "code",
        "outputId": "6ff6959d-c6d9-4926-f3ed-521a82d448b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 40\n",
        "# Print losses every N interations\n",
        "print_every_n = 50\n",
        "\n",
        "# Save every N iterations\n",
        "save_every_n = 200\n",
        "\n",
        "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
        "                lstm_size=lstm_size, num_layers=num_layers, \n",
        "                learning_rate=learning_rate)\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep=100)\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Use the line below to load a checkpoint and resume training\n",
        "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
        "    counter = 0\n",
        "    for e in range(epochs):\n",
        "        # Train network\n",
        "        new_state = sess.run(model.initial_state)\n",
        "        loss = 0\n",
        "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
        "            counter += 1\n",
        "            start = time.time()\n",
        "            feed = {model.inputs: x,\n",
        "                    model.targets: y,\n",
        "                    model.keep_prob: keep_prob,\n",
        "                    model.initial_state: new_state}\n",
        "            batch_loss, new_state, _ = sess.run([model.loss, \n",
        "                                                 model.final_state, \n",
        "                                                 model.optimizer], \n",
        "                                                 feed_dict=feed)\n",
        "            if (counter % print_every_n == 0):\n",
        "                end = time.time()\n",
        "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
        "                      'Training Step: {}... '.format(counter),\n",
        "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
        "                      '{:.4f} sec/batch'.format((end-start)))\n",
        "        \n",
        "            if (counter % save_every_n == 0):\n",
        "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
        "    \n",
        "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/40...  Training Step: 50...  Training loss: 3.0755...  0.1545 sec/batch\n",
            "Epoch: 1/40...  Training Step: 100...  Training loss: 2.5995...  0.1529 sec/batch\n",
            "Epoch: 1/40...  Training Step: 150...  Training loss: 2.3692...  0.1541 sec/batch\n",
            "Epoch: 2/40...  Training Step: 200...  Training loss: 2.1992...  0.1578 sec/batch\n",
            "Epoch: 2/40...  Training Step: 250...  Training loss: 2.0792...  0.1562 sec/batch\n",
            "Epoch: 2/40...  Training Step: 300...  Training loss: 1.9229...  0.1561 sec/batch\n",
            "Epoch: 2/40...  Training Step: 350...  Training loss: 1.8741...  0.1524 sec/batch\n",
            "Epoch: 3/40...  Training Step: 400...  Training loss: 1.7467...  0.1533 sec/batch\n",
            "Epoch: 3/40...  Training Step: 450...  Training loss: 1.6733...  0.1574 sec/batch\n",
            "Epoch: 3/40...  Training Step: 500...  Training loss: 1.6132...  0.1541 sec/batch\n",
            "Epoch: 3/40...  Training Step: 550...  Training loss: 1.5878...  0.1564 sec/batch\n",
            "Epoch: 4/40...  Training Step: 600...  Training loss: 1.5056...  0.1537 sec/batch\n",
            "Epoch: 4/40...  Training Step: 650...  Training loss: 1.5255...  0.1546 sec/batch\n",
            "Epoch: 4/40...  Training Step: 700...  Training loss: 1.4872...  0.1556 sec/batch\n",
            "Epoch: 4/40...  Training Step: 750...  Training loss: 1.4647...  0.1563 sec/batch\n",
            "Epoch: 5/40...  Training Step: 800...  Training loss: 1.4512...  0.1624 sec/batch\n",
            "Epoch: 5/40...  Training Step: 850...  Training loss: 1.4172...  0.1537 sec/batch\n",
            "Epoch: 5/40...  Training Step: 900...  Training loss: 1.4151...  0.1543 sec/batch\n",
            "Epoch: 5/40...  Training Step: 950...  Training loss: 1.3912...  0.1540 sec/batch\n",
            "Epoch: 6/40...  Training Step: 1000...  Training loss: 1.3749...  0.1560 sec/batch\n",
            "Epoch: 6/40...  Training Step: 1050...  Training loss: 1.4034...  0.1549 sec/batch\n",
            "Epoch: 6/40...  Training Step: 1100...  Training loss: 1.3700...  0.1625 sec/batch\n",
            "Epoch: 6/40...  Training Step: 1150...  Training loss: 1.3798...  0.1545 sec/batch\n",
            "Epoch: 7/40...  Training Step: 1200...  Training loss: 1.3244...  0.1534 sec/batch\n",
            "Epoch: 7/40...  Training Step: 1250...  Training loss: 1.3908...  0.1622 sec/batch\n",
            "Epoch: 7/40...  Training Step: 1300...  Training loss: 1.3131...  0.1553 sec/batch\n",
            "Epoch: 7/40...  Training Step: 1350...  Training loss: 1.3122...  0.1557 sec/batch\n",
            "Epoch: 8/40...  Training Step: 1400...  Training loss: 1.3238...  0.1535 sec/batch\n",
            "Epoch: 8/40...  Training Step: 1450...  Training loss: 1.3073...  0.1547 sec/batch\n",
            "Epoch: 8/40...  Training Step: 1500...  Training loss: 1.2722...  0.1558 sec/batch\n",
            "Epoch: 8/40...  Training Step: 1550...  Training loss: 1.2782...  0.1560 sec/batch\n",
            "Epoch: 9/40...  Training Step: 1600...  Training loss: 1.2412...  0.1567 sec/batch\n",
            "Epoch: 9/40...  Training Step: 1650...  Training loss: 1.2736...  0.1544 sec/batch\n",
            "Epoch: 9/40...  Training Step: 1700...  Training loss: 1.2300...  0.1565 sec/batch\n",
            "Epoch: 9/40...  Training Step: 1750...  Training loss: 1.2524...  0.1541 sec/batch\n",
            "Epoch: 10/40...  Training Step: 1800...  Training loss: 1.2740...  0.1542 sec/batch\n",
            "Epoch: 10/40...  Training Step: 1850...  Training loss: 1.2199...  0.1543 sec/batch\n",
            "Epoch: 10/40...  Training Step: 1900...  Training loss: 1.2523...  0.1537 sec/batch\n",
            "Epoch: 10/40...  Training Step: 1950...  Training loss: 1.3125...  0.1547 sec/batch\n",
            "Epoch: 11/40...  Training Step: 2000...  Training loss: 1.2553...  0.1546 sec/batch\n",
            "Epoch: 11/40...  Training Step: 2050...  Training loss: 1.2305...  0.1569 sec/batch\n",
            "Epoch: 11/40...  Training Step: 2100...  Training loss: 1.2267...  0.1559 sec/batch\n",
            "Epoch: 11/40...  Training Step: 2150...  Training loss: 1.2314...  0.1573 sec/batch\n",
            "Epoch: 12/40...  Training Step: 2200...  Training loss: 1.2252...  0.1551 sec/batch\n",
            "Epoch: 12/40...  Training Step: 2250...  Training loss: 1.2474...  0.1540 sec/batch\n",
            "Epoch: 12/40...  Training Step: 2300...  Training loss: 1.1806...  0.1524 sec/batch\n",
            "Epoch: 12/40...  Training Step: 2350...  Training loss: 1.2112...  0.1556 sec/batch\n",
            "Epoch: 13/40...  Training Step: 2400...  Training loss: 1.2200...  0.1550 sec/batch\n",
            "Epoch: 13/40...  Training Step: 2450...  Training loss: 1.1831...  0.1533 sec/batch\n",
            "Epoch: 13/40...  Training Step: 2500...  Training loss: 1.1928...  0.1546 sec/batch\n",
            "Epoch: 13/40...  Training Step: 2550...  Training loss: 1.2313...  0.1553 sec/batch\n",
            "Epoch: 14/40...  Training Step: 2600...  Training loss: 1.1668...  0.1563 sec/batch\n",
            "Epoch: 14/40...  Training Step: 2650...  Training loss: 1.2139...  0.1562 sec/batch\n",
            "Epoch: 14/40...  Training Step: 2700...  Training loss: 1.1532...  0.1546 sec/batch\n",
            "Epoch: 14/40...  Training Step: 2750...  Training loss: 1.1658...  0.1543 sec/batch\n",
            "Epoch: 15/40...  Training Step: 2800...  Training loss: 1.2136...  0.1560 sec/batch\n",
            "Epoch: 15/40...  Training Step: 2850...  Training loss: 1.1864...  0.1560 sec/batch\n",
            "Epoch: 15/40...  Training Step: 2900...  Training loss: 1.1918...  0.1553 sec/batch\n",
            "Epoch: 15/40...  Training Step: 2950...  Training loss: 1.2164...  0.1539 sec/batch\n",
            "Epoch: 16/40...  Training Step: 3000...  Training loss: 1.1882...  0.1521 sec/batch\n",
            "Epoch: 16/40...  Training Step: 3050...  Training loss: 1.1785...  0.1548 sec/batch\n",
            "Epoch: 16/40...  Training Step: 3100...  Training loss: 1.1451...  0.1630 sec/batch\n",
            "Epoch: 16/40...  Training Step: 3150...  Training loss: 1.1523...  0.1550 sec/batch\n",
            "Epoch: 17/40...  Training Step: 3200...  Training loss: 1.1470...  0.1533 sec/batch\n",
            "Epoch: 17/40...  Training Step: 3250...  Training loss: 1.1744...  0.1555 sec/batch\n",
            "Epoch: 17/40...  Training Step: 3300...  Training loss: 1.1567...  0.1551 sec/batch\n",
            "Epoch: 17/40...  Training Step: 3350...  Training loss: 1.1827...  0.1542 sec/batch\n",
            "Epoch: 18/40...  Training Step: 3400...  Training loss: 1.1754...  0.1544 sec/batch\n",
            "Epoch: 18/40...  Training Step: 3450...  Training loss: 1.1631...  0.1541 sec/batch\n",
            "Epoch: 18/40...  Training Step: 3500...  Training loss: 1.1728...  0.1534 sec/batch\n",
            "Epoch: 18/40...  Training Step: 3550...  Training loss: 1.1523...  0.1551 sec/batch\n",
            "Epoch: 19/40...  Training Step: 3600...  Training loss: 1.1696...  0.1558 sec/batch\n",
            "Epoch: 19/40...  Training Step: 3650...  Training loss: 1.1528...  0.1549 sec/batch\n",
            "Epoch: 19/40...  Training Step: 3700...  Training loss: 1.1573...  0.1574 sec/batch\n",
            "Epoch: 19/40...  Training Step: 3750...  Training loss: 1.1449...  0.1552 sec/batch\n",
            "Epoch: 20/40...  Training Step: 3800...  Training loss: 1.1259...  0.1524 sec/batch\n",
            "Epoch: 20/40...  Training Step: 3850...  Training loss: 1.1474...  0.1549 sec/batch\n",
            "Epoch: 20/40...  Training Step: 3900...  Training loss: 1.1686...  0.1546 sec/batch\n",
            "Epoch: 20/40...  Training Step: 3950...  Training loss: 1.1467...  0.1564 sec/batch\n",
            "Epoch: 21/40...  Training Step: 4000...  Training loss: 1.1433...  0.1538 sec/batch\n",
            "Epoch: 21/40...  Training Step: 4050...  Training loss: 1.1569...  0.1553 sec/batch\n",
            "Epoch: 21/40...  Training Step: 4100...  Training loss: 1.1516...  0.1542 sec/batch\n",
            "Epoch: 21/40...  Training Step: 4150...  Training loss: 1.1503...  0.1558 sec/batch\n",
            "Epoch: 22/40...  Training Step: 4200...  Training loss: 1.1873...  0.1623 sec/batch\n",
            "Epoch: 22/40...  Training Step: 4250...  Training loss: 1.1432...  0.1548 sec/batch\n",
            "Epoch: 22/40...  Training Step: 4300...  Training loss: 1.1582...  0.1535 sec/batch\n",
            "Epoch: 22/40...  Training Step: 4350...  Training loss: 1.1167...  0.1549 sec/batch\n",
            "Epoch: 23/40...  Training Step: 4400...  Training loss: 1.1261...  0.1544 sec/batch\n",
            "Epoch: 23/40...  Training Step: 4450...  Training loss: 1.1102...  0.1545 sec/batch\n",
            "Epoch: 23/40...  Training Step: 4500...  Training loss: 1.1751...  0.1545 sec/batch\n",
            "Epoch: 23/40...  Training Step: 4550...  Training loss: 1.1205...  0.1543 sec/batch\n",
            "Epoch: 24/40...  Training Step: 4600...  Training loss: 1.1256...  0.1591 sec/batch\n",
            "Epoch: 24/40...  Training Step: 4650...  Training loss: 1.1453...  0.1554 sec/batch\n",
            "Epoch: 24/40...  Training Step: 4700...  Training loss: 1.1648...  0.1536 sec/batch\n",
            "Epoch: 24/40...  Training Step: 4750...  Training loss: 1.1396...  0.1545 sec/batch\n",
            "Epoch: 25/40...  Training Step: 4800...  Training loss: 1.1320...  0.1537 sec/batch\n",
            "Epoch: 25/40...  Training Step: 4850...  Training loss: 1.1111...  0.1547 sec/batch\n",
            "Epoch: 25/40...  Training Step: 4900...  Training loss: 1.1519...  0.1548 sec/batch\n",
            "Epoch: 25/40...  Training Step: 4950...  Training loss: 1.2183...  0.1541 sec/batch\n",
            "Epoch: 26/40...  Training Step: 5000...  Training loss: 1.1417...  0.1527 sec/batch\n",
            "Epoch: 26/40...  Training Step: 5050...  Training loss: 1.1193...  0.1539 sec/batch\n",
            "Epoch: 26/40...  Training Step: 5100...  Training loss: 1.1124...  0.1540 sec/batch\n",
            "Epoch: 27/40...  Training Step: 5150...  Training loss: 1.1462...  0.1564 sec/batch\n",
            "Epoch: 27/40...  Training Step: 5200...  Training loss: 1.1570...  0.1535 sec/batch\n",
            "Epoch: 27/40...  Training Step: 5250...  Training loss: 1.1210...  0.1534 sec/batch\n",
            "Epoch: 27/40...  Training Step: 5300...  Training loss: 1.1007...  0.1572 sec/batch\n",
            "Epoch: 28/40...  Training Step: 5350...  Training loss: 1.1571...  0.1542 sec/batch\n",
            "Epoch: 28/40...  Training Step: 5400...  Training loss: 1.1412...  0.1544 sec/batch\n",
            "Epoch: 28/40...  Training Step: 5450...  Training loss: 1.1167...  0.1544 sec/batch\n",
            "Epoch: 28/40...  Training Step: 5500...  Training loss: 1.1141...  0.1524 sec/batch\n",
            "Epoch: 29/40...  Training Step: 5550...  Training loss: 1.0827...  0.1654 sec/batch\n",
            "Epoch: 29/40...  Training Step: 5600...  Training loss: 1.1269...  0.1554 sec/batch\n",
            "Epoch: 29/40...  Training Step: 5650...  Training loss: 1.1144...  0.1558 sec/batch\n",
            "Epoch: 29/40...  Training Step: 5700...  Training loss: 1.1246...  0.1556 sec/batch\n",
            "Epoch: 30/40...  Training Step: 5750...  Training loss: 1.1274...  0.1543 sec/batch\n",
            "Epoch: 30/40...  Training Step: 5800...  Training loss: 1.1127...  0.1545 sec/batch\n",
            "Epoch: 30/40...  Training Step: 5850...  Training loss: 1.1262...  0.1637 sec/batch\n",
            "Epoch: 30/40...  Training Step: 5900...  Training loss: 1.1141...  0.1553 sec/batch\n",
            "Epoch: 31/40...  Training Step: 5950...  Training loss: 1.1115...  0.1553 sec/batch\n",
            "Epoch: 31/40...  Training Step: 6000...  Training loss: 1.1475...  0.1531 sec/batch\n",
            "Epoch: 31/40...  Training Step: 6050...  Training loss: 1.1265...  0.1543 sec/batch\n",
            "Epoch: 31/40...  Training Step: 6100...  Training loss: 1.1341...  0.1545 sec/batch\n",
            "Epoch: 32/40...  Training Step: 6150...  Training loss: 1.1258...  0.1528 sec/batch\n",
            "Epoch: 32/40...  Training Step: 6200...  Training loss: 1.1596...  0.1545 sec/batch\n",
            "Epoch: 32/40...  Training Step: 6250...  Training loss: 1.1183...  0.1552 sec/batch\n",
            "Epoch: 32/40...  Training Step: 6300...  Training loss: 1.1179...  0.1556 sec/batch\n",
            "Epoch: 33/40...  Training Step: 6350...  Training loss: 1.1297...  0.1534 sec/batch\n",
            "Epoch: 33/40...  Training Step: 6400...  Training loss: 1.1216...  0.1620 sec/batch\n",
            "Epoch: 33/40...  Training Step: 6450...  Training loss: 1.1055...  0.1543 sec/batch\n",
            "Epoch: 33/40...  Training Step: 6500...  Training loss: 1.1099...  0.1543 sec/batch\n",
            "Epoch: 34/40...  Training Step: 6550...  Training loss: 1.0842...  0.1638 sec/batch\n",
            "Epoch: 34/40...  Training Step: 6600...  Training loss: 1.1065...  0.1535 sec/batch\n",
            "Epoch: 34/40...  Training Step: 6650...  Training loss: 1.0737...  0.1555 sec/batch\n",
            "Epoch: 34/40...  Training Step: 6700...  Training loss: 1.0988...  0.1624 sec/batch\n",
            "Epoch: 35/40...  Training Step: 6750...  Training loss: 1.1260...  0.1549 sec/batch\n",
            "Epoch: 35/40...  Training Step: 6800...  Training loss: 1.0981...  0.1544 sec/batch\n",
            "Epoch: 35/40...  Training Step: 6850...  Training loss: 1.1098...  0.1658 sec/batch\n",
            "Epoch: 35/40...  Training Step: 6900...  Training loss: 1.1454...  0.1554 sec/batch\n",
            "Epoch: 36/40...  Training Step: 6950...  Training loss: 1.1235...  0.1574 sec/batch\n",
            "Epoch: 36/40...  Training Step: 7000...  Training loss: 1.1044...  0.1658 sec/batch\n",
            "Epoch: 36/40...  Training Step: 7050...  Training loss: 1.1005...  0.1549 sec/batch\n",
            "Epoch: 36/40...  Training Step: 7100...  Training loss: 1.1029...  0.1534 sec/batch\n",
            "Epoch: 37/40...  Training Step: 7150...  Training loss: 1.1197...  0.1559 sec/batch\n",
            "Epoch: 37/40...  Training Step: 7200...  Training loss: 1.1168...  0.1550 sec/batch\n",
            "Epoch: 37/40...  Training Step: 7250...  Training loss: 1.0667...  0.1555 sec/batch\n",
            "Epoch: 37/40...  Training Step: 7300...  Training loss: 1.0892...  0.1527 sec/batch\n",
            "Epoch: 38/40...  Training Step: 7350...  Training loss: 1.1144...  0.1536 sec/batch\n",
            "Epoch: 38/40...  Training Step: 7400...  Training loss: 1.0809...  0.1556 sec/batch\n",
            "Epoch: 38/40...  Training Step: 7450...  Training loss: 1.0884...  0.1552 sec/batch\n",
            "Epoch: 38/40...  Training Step: 7500...  Training loss: 1.1066...  0.1532 sec/batch\n",
            "Epoch: 39/40...  Training Step: 7550...  Training loss: 1.0816...  0.1548 sec/batch\n",
            "Epoch: 39/40...  Training Step: 7600...  Training loss: 1.1027...  0.1667 sec/batch\n",
            "Epoch: 39/40...  Training Step: 7650...  Training loss: 1.0609...  0.1549 sec/batch\n",
            "Epoch: 39/40...  Training Step: 7700...  Training loss: 1.0775...  0.1549 sec/batch\n",
            "Epoch: 40/40...  Training Step: 7750...  Training loss: 1.1171...  0.1560 sec/batch\n",
            "Epoch: 40/40...  Training Step: 7800...  Training loss: 1.0966...  0.1600 sec/batch\n",
            "Epoch: 40/40...  Training Step: 7850...  Training loss: 1.0799...  0.1571 sec/batch\n",
            "Epoch: 40/40...  Training Step: 7900...  Training loss: 1.1208...  0.1616 sec/batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipul6IMo3vET",
        "colab_type": "code",
        "outputId": "0a3ae15e-46b5-4236-a550-a3f1194d00c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "tf.train.get_checkpoint_state('checkpoints')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model_checkpoint_path: \"checkpoints/i7920_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i1400_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i1600_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i2000_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i2200_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i2600_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i2800_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i3200_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i3600_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i3800_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i4000_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i4200_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i4400_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i4600_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i4800_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i5000_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i5200_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i5400_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i5600_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i5800_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i6000_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i6200_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i6400_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i6600_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i6800_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i7000_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i7200_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i7400_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i7600_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i7800_l512.ckpt\"\n",
              "all_model_checkpoint_paths: \"checkpoints/i7920_l512.ckpt\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-LHJzW83xVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pick_top_n(preds, vocab_size, top_n=5):\n",
        "    p = np.squeeze(preds)\n",
        "    p[np.argsort(p)[:-top_n]] = 0\n",
        "    p = p / np.sum(p)\n",
        "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
        "    return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPA4AbOb3zM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
        "    samples = [c for c in prime]\n",
        "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, checkpoint)\n",
        "        new_state = sess.run(model.initial_state)\n",
        "        for c in prime:\n",
        "            x = np.zeros((1, 1))\n",
        "            x[0,0] = vocab_to_int[c]\n",
        "            feed = {model.inputs: x,\n",
        "                    model.keep_prob: 1.,\n",
        "                    model.initial_state: new_state}\n",
        "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
        "                                         feed_dict=feed)\n",
        "\n",
        "        c = pick_top_n(preds, len(vocab))\n",
        "        samples.append(int_to_vocab[c])\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            x[0,0] = c\n",
        "            feed = {model.inputs: x,\n",
        "                    model.keep_prob: 1.,\n",
        "                    model.initial_state: new_state}\n",
        "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
        "                                         feed_dict=feed)\n",
        "\n",
        "            c = pick_top_n(preds, len(vocab))\n",
        "            samples.append(int_to_vocab[c])\n",
        "        \n",
        "    return ''.join(samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW-KGzlU36N6",
        "colab_type": "code",
        "outputId": "587796ed-3832-47ec-87b9-ff944ab604b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint('checkpoints')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'checkpoints/i7920_l512.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4I765ih39De",
        "colab_type": "code",
        "outputId": "4c51e03b-e422-4151-9087-48f7a0774cc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        }
      },
      "source": [
        "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
        "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
        "print(samp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Far too happy,\n",
            "sometimes, the second same word. Sergey Ivanovitch's attitude to my\n",
            "husband, and was always serious one. And at that moment, he could not\n",
            "torching his feigher to be so awful and talking. And he was in an\n",
            "insult to her; all he wanted to say that it would be so much as so struggles\n",
            "in them into that servant, and all had been a stronger friends with\n",
            "his first arrangement.\n",
            "\n",
            "Then he was not saying that it was awaiting herself in him that how he\n",
            "was something and had time to be alone. She did not say that he could not\n",
            "tell the same, and he could not have said, and the paint had an association\n",
            "that she was thinking.\n",
            "\n",
            "\"You will never have sooned things and then and the doctor, and I will\n",
            "beal to myself,\" said the princess. All the clight of the stations of\n",
            "their attitude, and the commenculated stuffing at a caressing of action,\n",
            "she felt at a man, and the same, with the candiduties of the sight\n",
            "of his feeling of accordance as he said, she was talking to her. He\n",
            "stared at her son with his shilled and said, and at those position had\n",
            "gone on any other she was anxious, and had been starting the theeps\n",
            "of his soul of the chief sack of the professor who had seemed to him\n",
            "to hide her hands, but to herself his family and their atchate and\n",
            "starting.\n",
            "\n",
            "This consenting women was. The sound was impossible, and would not be\n",
            "done to tell her, and he wanted to be sent to the meaning of a\n",
            "state, he had brought a going, and still all the children his\n",
            "strine was important to her.\n",
            "\n",
            "\"I conceal her, that's all along that when.... That is,\" she said,\n",
            "and all those heart were satisfied and through his sister and to\n",
            "bring at the station of this attention to Anna.\n",
            "\n",
            "\"When should you see her? Alone and then the strange sense of him,\" answered\n",
            "the prince.\n",
            "\n",
            "\"Ah! that was a minute,\" she answered.\n",
            "\n",
            "\"A sister was all the particular actions. I was satisfactory!\" answered\n",
            "Vronsky. \"What is she breathing,\" answered Vassenka Veslovsky, and his\n",
            "brother's side and seemed as always.\n",
            "\n",
            "\"I don't want to be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCigcMpy4ByP",
        "colab_type": "code",
        "outputId": "58e303ed-3e7d-4797-f382-1dbd43872885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
        "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
        "print(samp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Far s whet har ans\n",
            "on shit she che wers the wer that\n",
            "hat whe sass of wer sam the\n",
            "the hav on he wat hat and and at of tho shat his,\"\n",
            "sand sow the corser tere her as had, ald him went ant the pind, at the she the somtant and the\n",
            "sand wink the pith the was the\n",
            "galy wish and her so dit ho dessers, wor had\n",
            "wass, sho canles wouk, she has would shers of the pot time hers to tiste hord an to him to the wels was a de that his whonge she tor to se shither she wass he tong and will had, wat her and thom, stout tho the\n",
            "prace an her he wise he was him\n",
            "astols and the cas to the sass the sors he wort te that he sat he the se tit the wind\n",
            "the co th at a pon werting whe sond on\n",
            "his sill with he se tham the what ald the were and hat a dads and hid, and the hers tore the pots on the she songhered whet his as the had him\n",
            "sand, and the sad so whin wisted at wime and and he wamle a sow te weme the sald\n",
            "the should was tors at thime a sout of he tow\n",
            "wom to drind, and her than he whet the were sise was\n",
            "ang the pet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIrXe3gC4EmX",
        "colab_type": "code",
        "outputId": "dbb4454a-e58f-4b96-aa78-f9ae2801eb04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
        "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far the once\")\n",
        "print(samp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Far the once, and she was not in the position to anywered in\n",
            "the servinch at him, and winter hand.\n",
            "\n",
            "Stryou saw all a chattion with anyone of\n",
            "the meatings. \"I am shalt the some only\n",
            "to the must was in told him?\"\n",
            "\n",
            "\"I say that this asticing that serving home.\"\n",
            "\n",
            "The sour. She seived at a stalk of her was and a convisious to\n",
            "the more to the pare a stopped that she\n",
            "was there was than a feeling had\n",
            "been them, and were\n",
            "to glance has\n",
            "step the mosticition of the possion.\n",
            "\n",
            "The sain it, and the madine, sat that had setting to his hands about something of the propling when himself that her same as a mon in the serval,\n",
            "want officure.\n",
            "\n",
            "\"I would not speek and an too.\"\n",
            "\n",
            "\"Well, what have she's thought with him out of the\n",
            "shalled\n",
            "of the cordier. He don't gres, I have not askow all his sighel stards of this howen and stall him of himself of the placile they heard seeing tomeress, the persons. There have think of howers. This say\n",
            "to\n",
            "the mast on\n",
            "this was still been she\n",
            "could not saw, at the\n",
            "constant of his, and his face\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPTHxaLf4H9t",
        "colab_type": "code",
        "outputId": "5b2dcb00-211c-4a7f-cc20-aedb3985007e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
        "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
        "print(samp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Farvata over the most course the\n",
            "mother, still te hear of the part, and\n",
            "somewhing astachering, that the master was almost to show the carriage with the work this he would not consider a smile, and\n",
            "with her satisfactes, with a finish of the marriage, and a from with a little compance, they\n",
            "was and was a share as they went to take this women was the statefully as to the position which had been words, though\n",
            "it were to have taken into the manself, and shreek him with\n",
            "themself as intolerately so intensed, to thong, which set to\n",
            "the conversation to the prince shatched her hand. She had no despit men her sisters, he felt he came at\n",
            "her single, as he would be so fond about their sacrearing in the mother, and they went\n",
            "out.\n",
            "\n",
            "\"Only that's a people think is the might see it all the position in her and there will say them. He wanted to too\n",
            "to hear, how if you said that I am\n",
            "not tone in the seet of their man, when I want to\n",
            "hear to string that it is a finging to the pluce of her simples than all, woul\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
